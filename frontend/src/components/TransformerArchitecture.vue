<template>
  <div class="transformer-container">
    <h2 class="transformer-title">Transformer æ¶æ„è¯¦è§£</h2>
    <p class="transformer-subtitle">æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é©å‘½æ€§æ¶æ„ - ä»è®­ç»ƒåˆ°æ¨ç†çš„å®Œæ•´è§£æ</p>

    <!-- æ¦‚è¿°éƒ¨åˆ† -->
    <section class="section overview">
      <h3>ä»€ä¹ˆæ˜¯Transformerï¼Ÿ</h3>
      <div class="content">
        <p>
          Transformeræ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ï¼Œé¦–æ¬¡åœ¨è®ºæ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºã€‚
          å®ƒå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿçš„å¾ªç¯å’Œå·ç§¯ç»“æ„ï¼Œæˆä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„ã€‚
        </p>
        <div class="architecture-diagram">
          <h4>Transformeræ•´ä½“æ¶æ„</h4>
          <div class="diagram-container">
            <div class="transformer-flow">
              <div class="input-section">
                <div class="input-tokens">
                  <span class="token">The</span>
                  <span class="token">Transformer</span>
                  <span class="token">model</span>
                  <span class="token">is</span>
                  <span class="token">powerful</span>
                </div>
                <div class="input-label">è¾“å…¥åºåˆ—</div>
              </div>
              
              <div class="encoder-stack">
                <div class="encoder-header">ç¼–ç å™¨å †å  (N=6)</div>
                <div class="encoder-blocks">
                  <div class="encoder-block" v-for="i in 6" :key="'enc-' + i">
                    <div class="block-title">ç¼–ç å™¨å— {{i}}</div>
                    <div class="attention-layer">
                      <div class="layer-name">å¤šå¤´è‡ªæ³¨æ„åŠ›</div>
                      <div class="attention-connections">
                        <div class="conn-row" v-for="j in 5" :key="'conn-' + j">
                          <div class="conn-cell" v-for="k in 5" :key="'cell-' + k" 
                               :class="{ 'active': j === k || i % 2 === 0 }">
                            <div class="conn-dot"></div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="ffn-layer">
                      <div class="layer-name">å‰é¦ˆç½‘ç»œ</div>
                    </div>
                  </div>
                </div>
              </div>
              
              <div class="decoder-stack">
                <div class="decoder-header">è§£ç å™¨å †å  (N=6)</div>
                <div class="decoder-blocks">
                  <div class="decoder-block" v-for="i in 6" :key="'dec-' + i">
                    <div class="block-title">è§£ç å™¨å— {{i}}</div>
                    <div class="masked-attention-layer">
                      <div class="layer-name">æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›</div>
                    </div>
                    <div class="cross-attention-layer">
                      <div class="layer-name">ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›</div>
                    </div>
                    <div class="ffn-layer">
                      <div class="layer-name">å‰é¦ˆç½‘ç»œ</div>
                    </div>
                  </div>
                </div>
              </div>
              
              <div class="output-section">
                <div class="output-label">è¾“å‡ºåºåˆ—</div>
                <div class="output-tokens">
                  <span class="token">Le</span>
                  <span class="token">modÃ¨le</span>
                  <span class="token">Transformer</span>
                  <span class="token">est</span>
                  <span class="token">puissant</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- è®­ç»ƒè¿‡ç¨‹ -->
    <section class="section training">
      <h3>è®­ç»ƒè¿‡ç¨‹</h3>
      <div class="content">
        <p>
          Transformerçš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šå­¦ä¹ è¯­è¨€è¡¨ç¤ºã€‚
          åœ¨å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒã€‚
        </p>
        <div class="training-process">
          <div class="process-visualization">
            <div class="data-flow">
              <div class="data-step">1. <strong>æ•°æ®è¾“å…¥</strong>: å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®</div>
              <div class="data-step">2. <strong>TokenåŒ–</strong>: æ–‡æœ¬è½¬ä¸ºæ•°å­—åºåˆ—</div>
              <div class="data-step">3. <strong>ä½ç½®ç¼–ç </strong>: æ·»åŠ ä½ç½®ä¿¡æ¯</div>
              <div class="data-step">4. <strong>æ¨¡å‹å¤„ç†</strong>: ç¼–ç å™¨-è§£ç å™¨å¤„ç†</div>
              <div class="data-step">5. <strong>æŸå¤±è®¡ç®—</strong>: é¢„æµ‹ä¸å®é™…æ¯”è¾ƒ</div>
              <div class="data-step">6. <strong>å‚æ•°æ›´æ–°</strong>: åå‘ä¼ æ’­ä¼˜åŒ–</div>
            </div>
          </div>
        </div>
        <div class="training-details">
          <h4>è®­ç»ƒç»†èŠ‚</h4>
          <ul>
            <li><strong>æŸå¤±å‡½æ•°</strong>: é€šå¸¸ä½¿ç”¨äº¤å‰ç†µæŸå¤±</li>
            <li><strong>ä¼˜åŒ–å™¨</strong>: Adamä¼˜åŒ–å™¨ï¼Œå¸¦å­¦ä¹ ç‡é¢„çƒ­</li>
            <li><strong>å¹¶è¡Œå¤„ç†</strong>: æ‰¹é‡å¤„ç†åºåˆ—ä¸­çš„æ‰€æœ‰token</li>
            <li><strong>é¢„è®­ç»ƒä»»åŠ¡</strong>: æ©ç è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥é¢„æµ‹</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- æ¨ç†è¿‡ç¨‹ -->
    <section class="section inference">
      <h3>æ¨ç†è¿‡ç¨‹</h3>
      <div class="content">
        <p>
          åœ¨æ¨ç†é˜¶æ®µï¼ŒTransformeræ¨¡å‹æ¥æ”¶è¾“å…¥åºåˆ—å¹¶ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚å¯¹äºè‡ªå›å½’ç”Ÿæˆï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆï¼‰ï¼Œ
          æ¨¡å‹ä¼šé€æ­¥ç”Ÿæˆè¾“å‡ºtokenï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªtokenå¹¶å°†å…¶ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥ã€‚
        </p>
        <div class="inference-process">
          <div class="process-steps">
            <div class="step">
              <div class="step-icon">ğŸ“¥</div>
              <h4>è¾“å…¥ç¼–ç </h4>
              <p>è¾“å…¥åºåˆ—é€šè¿‡ç¼–ç å™¨ç¼–ç ä¸ºè¡¨ç¤ºå‘é‡</p>
            </div>
            <div class="step">
              <div class="step-icon">ğŸ”„</div>
              <h4>è‡ªå›å½’ç”Ÿæˆ</h4>
              <p>è§£ç å™¨é€æ­¥ç”Ÿæˆè¾“å‡ºï¼Œæ¯æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ªtoken</p>
            </div>
            <div class="step">
              <div class="step-icon">ğŸ“Š</div>
              <h4>æ¦‚ç‡è®¡ç®—</h4>
              <p>è®¡ç®—è¯æ±‡è¡¨ä¸­æ¯ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ</p>
            </div>
            <div class="step">
              <div class="step-icon">ğŸ¯</div>
              <h4>é‡‡æ ·ç­–ç•¥</h4>
              <p>ä½¿ç”¨è´ªå©ªæœç´¢ã€æŸæœç´¢æˆ–éšæœºé‡‡æ ·é€‰æ‹©è¾“å‡ºtoken</p>
            </div>
          </div>
        </div>
        <div class="inference-details">
          <h4>æ¨ç†ç­–ç•¥</h4>
          <ul>
            <li><strong>è´ªå©ªæœç´¢</strong>: æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token</li>
            <li><strong>æŸæœç´¢</strong>: ä¿ç•™å¤šä¸ªå€™é€‰åºåˆ—ï¼Œé€‰æ‹©æ•´ä½“æœ€ä¼˜</li>
            <li><strong>éšæœºé‡‡æ ·</strong>: æ ¹æ®æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©token</li>
            <li><strong>Top-k/Top-pé‡‡æ ·</strong>: é™åˆ¶é‡‡æ ·èŒƒå›´ä»¥å¹³è¡¡åˆ›é€ æ€§ä¸è´¨é‡</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- æ ¸å¿ƒç»„ä»¶ -->
    <section class="section components">
      <h3>æ ¸å¿ƒç»„ä»¶è¯¦è§£</h3>
      <div class="content">
        <div class="component-grid">
          <div class="component">
            <h4>å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶</h4>
            <p>
              é€šè¿‡çº¿æ€§å˜æ¢ç”ŸæˆQã€Kã€VçŸ©é˜µï¼Œè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œå®ç°å¯¹è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®çš„å…³è”ã€‚
              å…¬å¼ï¼šAttention(Q, K, V) = softmax(QK^T / âˆšd_k)V
            </p>
          </div>
          <div class="component">
            <h4>ä½ç½®ç¼–ç </h4>
            <p>
              ç”±äºTransformeræ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œéœ€è¦æ·»åŠ ä½ç½®ç¼–ç æ¥ä¿ç•™åºåˆ—é¡ºåºä¿¡æ¯ã€‚
              é€šå¸¸ä½¿ç”¨æ­£å¼¦/ä½™å¼¦å‡½æ•°æˆ–å­¦ä¹ çš„ä½ç½®åµŒå…¥ã€‚
            </p>
          </div>
          <div class="component">
            <h4>å‰é¦ˆç½‘ç»œ</h4>
            <p>
              ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªæ¿€æ´»å‡½æ•°ç»„æˆçš„å…¨è¿æ¥ç½‘ç»œï¼Œåº”ç”¨äºæ¯ä¸ªä½ç½®çš„è¡¨ç¤ºã€‚
              é€šå¸¸ä½¿ç”¨ReLUæˆ–GELUæ¿€æ´»å‡½æ•°ã€‚
            </p>
          </div>
          <div class="component">
            <h4>æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–</h4>
            <p>
              åœ¨æ¯ä¸ªå­å±‚åæ·»åŠ æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºæ¢¯åº¦ä¼ æ’­å’Œè®­ç»ƒç¨³å®šæ€§ã€‚
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- ä»£ç ç¤ºä¾‹ -->
    <section class="section code">
      <h3>æ ¸å¿ƒä»£ç ç¤ºä¾‹</h3>
      <div class="content">
        <div class="code-container">
          <div class="code-block">
            <h4>å¤šå¤´æ³¨æ„åŠ›å®ç°</h4>
            <pre><code>class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        matmul_qk = torch.matmul(Q, K.transpose(-2, -1))
        dk = K.size()[-1]
        scaled_attention_logits = matmul_qk / math.sqrt(dk)
        
        if mask is not None:
            scaled_attention_logits += (mask * -1e9)
            
        attention_weights = F.softmax(scaled_attention_logits, dim=-1)
        output = torch.matmul(attention_weights, V)
        return output
        
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        scaled_attention = self.scaled_dot_product_attention(Q, K, V, mask)
        concat_attention = scaled_attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        output = self.W_o(concat_attention)
        return output</code></pre>
          </div>
          
          <div class="code-block">
            <h4>ä½ç½®ç¼–ç å®ç°</h4>
            <pre><code>class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                            -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]</code></pre>
          </div>
          
          <div class="code-block">
            <h4>å®Œæ•´Transformerå®ç°</h4>
            <pre><code>class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout):
        super(Transformer, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        
        # è¯åµŒå…¥å±‚
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(d_model, max_len)
        
        # ç¼–ç å™¨å’Œè§£ç å™¨å±‚
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)
        ])
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.dropout = nn.Dropout(dropout)
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        # æºåºåˆ—åµŒå…¥å’Œä½ç½®ç¼–ç 
        src_embedding = self.dropout(self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model)))
        
        # ç›®æ ‡åºåˆ—åµŒå…¥å’Œä½ç½®ç¼–ç 
        tgt_embedding = self.dropout(self.positional_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model)))
        
        # ç¼–ç å™¨å¤„ç†
        enc_output = src_embedding
        for enc_layer in self.encoder_layers:
            enc_output = enc_layer(enc_output, src_mask)
        
        # è§£ç å™¨å¤„ç†
        dec_output = tgt_embedding
        for dec_layer in self.decoder_layers:
            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)
        
        # è¾“å‡ºå±‚
        output = self.fc_out(dec_output)
        return output</code></pre>
          </div>
        </div>
      </div>
    </section>
  </div>
</template>

<script>
export default {
  name: 'TransformerArchitecture',
  data() {
    return {
      // ç»„ä»¶æ•°æ®
    };
  },
  methods: {
    // ç»„ä»¶æ–¹æ³•
  }
};
</script>

<style scoped>
.transformer-container {
  max-width: 1400px;
  margin: 0 auto;
  padding: 30px;
  color: #e0e0e0;
  background: rgba(10, 14, 39, 0.8);
  border: 1px solid rgba(0, 242, 255, 0.2);
  border-radius: 16px;
  backdrop-filter: blur(10px);
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
}

.transformer-title {
  text-align: center;
  font-size: 38px;
  color: #00f2ff;
  margin-bottom: 15px;
  font-weight: 700;
  text-shadow: 0 0 15px rgba(0, 242, 255, 0.5);
}

.transformer-subtitle {
  text-align: center;
  font-size: 18px;
  color: #8899aa;
  margin-bottom: 40px;
}

.section {
  margin-bottom: 40px;
  padding: 25px;
  background: rgba(0, 0, 0, 0.2);
  border: 1px solid rgba(0, 242, 255, 0.1);
  border-radius: 12px;
  box-shadow: 0 4px 16px rgba(0, 0, 0, 0.2);
}

.section h3 {
  font-size: 26px;
  color: #00f2ff;
  margin-bottom: 20px;
  font-weight: 600;
}

.content {
  line-height: 1.8;
}

.content p {
  margin-bottom: 20px;
  font-size: 16px;
}

.architecture-diagram {
  margin-top: 20px;
}

.diagram-container {
  background: rgba(0, 0, 0, 0.3);
  padding: 20px;
  border-radius: 8px;
  overflow-x: auto;
}

.encoder-decoder {
  display: flex;
  gap: 20px;
  min-width: 800px;
}

.encoder, .decoder {
  flex: 1;
  padding: 15px;
  border: 1px solid rgba(0, 242, 255, 0.3);
  border-radius: 8px;
  background: rgba(0, 30, 60, 0.4);
}

.encoder h5, .decoder h5 {
  text-align: center;
  color: #51cf66;
  margin-bottom: 15px;
  font-size: 20px;
}

.blocks {
  display: flex;
  flex-direction: column;
  gap: 10px;
}

.block {
  padding: 12px;
  border: 1px solid rgba(100, 200, 255, 0.3);
  border-radius: 6px;
  background: rgba(0, 40, 80, 0.4);
}

.block-label {
  display: block;
  text-align: center;
  font-weight: bold;
  color: #ffd43b;
  margin-bottom: 8px;
}

.sublayers {
  display: flex;
  flex-direction: column;
  gap: 5px;
}

.sublayer {
  padding: 6px;
  background: rgba(100, 150, 255, 0.2);
  border: 1px solid rgba(100, 150, 255, 0.4);
  border-radius: 4px;
  text-align: center;
  font-size: 14px;
}

.process-steps, .inference-steps {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
  gap: 20px;
  margin-top: 20px;
}

.step {
  padding: 20px;
  background: rgba(0, 40, 80, 0.3);
  border: 1px solid rgba(0, 150, 200, 0.3);
  border-radius: 8px;
}

.step h4 {
  color: #4dabf7;
  margin-bottom: 10px;
  font-size: 18px;
}

.component-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
  gap: 20px;
}

.component {
  padding: 20px;
  background: rgba(60, 0, 80, 0.2);
  border: 1px solid rgba(150, 100, 255, 0.3);
  border-radius: 8px;
}

.component h4 {
  color: #9775fa;
  margin-bottom: 10px;
  font-size: 18px;
}

.code-blocks {
  display: flex;
  flex-direction: column;
  gap: 20px;
}

.code-block {
  background: rgba(0, 0, 0, 0.4);
  border: 1px solid rgba(100, 200, 255, 0.3);
  border-radius: 8px;
  overflow: hidden;
}

.code-block h4 {
  padding: 12px 20px;
  background: rgba(0, 30, 60, 0.6);
  color: #ff6b6b;
  margin: 0;
  font-size: 18px;
}

pre {
  margin: 0;
  padding: 20px;
  overflow-x: auto;
  background: rgba(0, 0, 0, 0.5);
  color: #f8f8f2;
  font-size: 14px;
  line-height: 1.5;
}

@media (max-width: 768px) {
  .transformer-container {
    padding: 15px;
  }
  
  .transformer-title {
    font-size: 28px;
  }
  
  .encoder-decoder {
    flex-direction: column;
    min-width: auto;
  }
  
  .process-steps, .inference-steps, .component-grid {
    grid-template-columns: 1fr;
  }
}
</style>