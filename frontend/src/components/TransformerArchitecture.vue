<template>
  <div class="transformer-container">
    <h2 class="transformer-title">Transformer æ¶æ„è¯¦è§£</h2>
    <p class="transformer-subtitle">æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é©å‘½æ€§æ¶æ„ - ä»è®­ç»ƒåˆ°æ¨ç†çš„å®Œæ•´è§£æ</p>

    <!-- æ¦‚è¿°éƒ¨åˆ† -->
    <section class="section overview">
      <h3>ä»€ä¹ˆæ˜¯Transformerï¼Ÿ</h3>
      <div class="content">
        <p>
          Transformeræ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ï¼Œé¦–æ¬¡åœ¨è®ºæ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºã€‚
          å®ƒå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿçš„å¾ªç¯å’Œå·ç§¯ç»“æ„ï¼Œæˆä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„ã€‚
        </p>
        <div class="architecture-diagram">
          <h4>Transformeræ•´ä½“æ¶æ„</h4>
          <div class="diagram-container">
            <div class="transformer-flow">
              <div class="input-section">
                <div class="input-tokens">
                  <span class="token">The</span>
                  <span class="token">Transformer</span>
                  <span class="token">model</span>
                  <span class="token">is</span>
                  <span class="token">powerful</span>
                </div>
                <div class="input-label">è¾“å…¥åºåˆ—</div>
              </div>
              
              <div class="encoder-stack">
                <div class="encoder-header">ç¼–ç å™¨å †å  (N=6)</div>
                <div class="encoder-blocks">
                  <div class="encoder-block" v-for="i in 6" :key="'enc-' + i">
                    <div class="block-title">ç¼–ç å™¨å— {{i}}</div>
                    <div class="attention-layer">
                      <div class="layer-name">å¤šå¤´è‡ªæ³¨æ„åŠ›</div>
                      <div class="attention-connections">
                        <div class="conn-row" v-for="j in 5" :key="'conn-' + j">
                          <div class="conn-cell" v-for="k in 5" :key="'cell-' + k" 
                               :class="{ 'active': j === k || i % 2 === 0 }">
                            <div class="conn-dot"></div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="ffn-layer">
                      <div class="layer-name">å‰é¦ˆç½‘ç»œ</div>
                    </div>
                  </div>
                </div>
              </div>
              
              <div class="decoder-stack">
                <div class="decoder-header">è§£ç å™¨å †å  (N=6)</div>
                <div class="decoder-blocks">
                  <div class="decoder-block" v-for="i in 6" :key="'dec-' + i">
                    <div class="block-title">è§£ç å™¨å— {{i}}</div>
                    <div class="masked-attention-layer">
                      <div class="layer-name">æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›</div>
                    </div>
                    <div class="cross-attention-layer">
                      <div class="layer-name">ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›</div>
                    </div>
                    <div class="ffn-layer">
                      <div class="layer-name">å‰é¦ˆç½‘ç»œ</div>
                    </div>
                  </div>
                </div>
              </div>
              
              <div class="output-section">
                <div class="output-label">è¾“å‡ºåºåˆ—</div>
                <div class="output-tokens">
                  <span class="token">Le</span>
                  <span class="token">modÃ¨le</span>
                  <span class="token">Transformer</span>
                  <span class="token">est</span>
                  <span class="token">puissant</span>
                </div>
              </div>
            </div>

            <!-- æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ– -->
            <div class="attention-chart">
              <AttentionVisualization :attention-weights="attentionWeights" />
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- è®­ç»ƒè¿‡ç¨‹ -->
    <section class="section training">
      <h3>è®­ç»ƒè¿‡ç¨‹</h3>
      <div class="content">
        <p>
          Transformerçš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šå­¦ä¹ è¯­è¨€è¡¨ç¤ºã€‚
          åœ¨å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒã€‚
        </p>
        <div class="training-process">
          <div class="timeline-header">
            <h4>è®­ç»ƒæµç¨‹</h4>
            <div class="timeline-controls">
              <button class="control-btn" @click="startTrainingAnimation">
                <span v-if="!isTrainingActive">â–¶ æ’­æ”¾åŠ¨ç”»</span>
                <span v-else>â¸ æš‚åœ</span>
              </button>
            </div>
          </div>
          <div class="timeline-container training-timeline">
            <div class="timeline-line">
              <div class="timeline-progress"
                   :style="{ width: trainingProgress + '%' }"></div>
            </div>
            <div class="timeline-steps">
              <div class="timeline-step"
                   v-for="(step, index) in trainingSteps"
                   :key="index"
                   :class="{
                     'active': index <= currentTrainingStep && isTrainingActive,
                     'completed': index < currentTrainingStep
                   }"
                   @click="selectTrainingStep(index)">
                <div class="step-number">{{ index + 1 }}</div>
                <div class="step-content">
                  <h4>{{ step.title }}</h4>
                  <p>{{ step.desc }}</p>
                </div>
                <div class="step-connector">
                  <div class="connector-line"></div>
                  <div class="connector-arrow">âœ</div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="training-details">
          <h4>è®­ç»ƒç»†èŠ‚</h4>
          <ul>
            <li><strong>æŸå¤±å‡½æ•°</strong>: é€šå¸¸ä½¿ç”¨äº¤å‰ç†µæŸå¤±</li>
            <li><strong>ä¼˜åŒ–å™¨</strong>: Adamä¼˜åŒ–å™¨ï¼Œå¸¦å­¦ä¹ ç‡é¢„çƒ­</li>
            <li><strong>å¹¶è¡Œå¤„ç†</strong>: æ‰¹é‡å¤„ç†åºåˆ—ä¸­çš„æ‰€æœ‰token</li>
            <li><strong>é¢„è®­ç»ƒä»»åŠ¡</strong>: æ©ç è¯­è¨€å»ºæ¨¡æˆ–ä¸‹ä¸€å¥é¢„æµ‹</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- æ¨ç†è¿‡ç¨‹ -->
    <section class="section inference">
      <h3>æ¨ç†è¿‡ç¨‹</h3>
      <div class="content">
        <p>
          åœ¨æ¨ç†é˜¶æ®µï¼ŒTransformeræ¨¡å‹æ¥æ”¶è¾“å…¥åºåˆ—å¹¶ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚å¯¹äºè‡ªå›å½’ç”Ÿæˆï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆï¼‰ï¼Œ
          æ¨¡å‹ä¼šé€æ­¥ç”Ÿæˆè¾“å‡ºtokenï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªtokenå¹¶å°†å…¶ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥ã€‚
        </p>
        <div class="inference-process">
          <div class="timeline-header">
            <h4>æ¨ç†æµç¨‹</h4>
            <div class="timeline-controls">
              <button class="control-btn" @click="startInferenceAnimation">
                <span v-if="!isInferenceActive">â–¶ æ’­æ”¾åŠ¨ç”»</span>
                <span v-else>â¸ æš‚åœ</span>
              </button>
            </div>
          </div>
          <div class="timeline-container inference-timeline">
            <div class="timeline-line">
              <div class="timeline-progress"
                   :style="{ width: inferenceProgress + '%' }"></div>
            </div>
            <div class="timeline-steps">
              <div class="timeline-step"
                   v-for="(step, index) in inferenceSteps"
                   :key="index"
                   :class="{
                     'active': index <= currentInferenceStep && isInferenceActive,
                     'completed': index < currentInferenceStep
                   }"
                   @click="selectInferenceStep(index)">
                <div class="step-icon">{{ step.icon }}</div>
                <div class="step-content">
                  <h4>{{ step.title }}</h4>
                  <p>{{ step.desc }}</p>
                </div>
                <div class="step-connector">
                  <div class="connector-line"></div>
                  <div class="connector-arrow">âœ</div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="inference-details">
          <h4>æ¨ç†ç­–ç•¥</h4>
          <ul>
            <li><strong>è´ªå©ªæœç´¢</strong>: æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token</li>
            <li><strong>æŸæœç´¢</strong>: ä¿ç•™å¤šä¸ªå€™é€‰åºåˆ—ï¼Œé€‰æ‹©æ•´ä½“æœ€ä¼˜</li>
            <li><strong>éšæœºé‡‡æ ·</strong>: æ ¹æ®æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©token</li>
            <li><strong>Top-k/Top-pé‡‡æ ·</strong>: é™åˆ¶é‡‡æ ·èŒƒå›´ä»¥å¹³è¡¡åˆ›é€ æ€§ä¸è´¨é‡</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- æ ¸å¿ƒç»„ä»¶ -->
    <section class="section components">
      <h3>æ ¸å¿ƒç»„ä»¶è¯¦è§£</h3>
      <div class="content">
        <div class="component-grid">
          <div class="component">
            <h4>å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶</h4>
            <p>
              é€šè¿‡çº¿æ€§å˜æ¢ç”ŸæˆQã€Kã€VçŸ©é˜µï¼Œè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œå®ç°å¯¹è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®çš„å…³è”ã€‚
              å…¬å¼ï¼šAttention(Q, K, V) = softmax(QK^T / âˆšd_k)V
            </p>
          </div>
          <div class="component">
            <h4>ä½ç½®ç¼–ç </h4>
            <p>
              ç”±äºTransformeræ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œéœ€è¦æ·»åŠ ä½ç½®ç¼–ç æ¥ä¿ç•™åºåˆ—é¡ºåºä¿¡æ¯ã€‚
              é€šå¸¸ä½¿ç”¨æ­£å¼¦/ä½™å¼¦å‡½æ•°æˆ–å­¦ä¹ çš„ä½ç½®åµŒå…¥ã€‚
            </p>
          </div>
          <div class="component">
            <h4>å‰é¦ˆç½‘ç»œ</h4>
            <p>
              ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªæ¿€æ´»å‡½æ•°ç»„æˆçš„å…¨è¿æ¥ç½‘ç»œï¼Œåº”ç”¨äºæ¯ä¸ªä½ç½®çš„è¡¨ç¤ºã€‚
              é€šå¸¸ä½¿ç”¨ReLUæˆ–GELUæ¿€æ´»å‡½æ•°ã€‚
            </p>
          </div>
          <div class="component">
            <h4>æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–</h4>
            <p>
              åœ¨æ¯ä¸ªå­å±‚åæ·»åŠ æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºæ¢¯åº¦ä¼ æ’­å’Œè®­ç»ƒç¨³å®šæ€§ã€‚
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- ä»£ç ç¤ºä¾‹ -->
    <section class="section code">
      <h3>æ ¸å¿ƒä»£ç ç¤ºä¾‹</h3>
      <div class="content">
        <div class="code-container">
          <div class="code-block">
            <h4>å¤šå¤´æ³¨æ„åŠ›å®ç°</h4>
            <pre><code>class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        matmul_qk = torch.matmul(Q, K.transpose(-2, -1))
        dk = K.size()[-1]
        scaled_attention_logits = matmul_qk / math.sqrt(dk)
        
        if mask is not None:
            scaled_attention_logits += (mask * -1e9)
            
        attention_weights = F.softmax(scaled_attention_logits, dim=-1)
        output = torch.matmul(attention_weights, V)
        return output
        
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        scaled_attention = self.scaled_dot_product_attention(Q, K, V, mask)
        concat_attention = scaled_attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        output = self.W_o(concat_attention)
        return output</code></pre>
          </div>
          
          <div class="code-block">
            <h4>ä½ç½®ç¼–ç å®ç°</h4>
            <pre><code>class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                            -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]</code></pre>
          </div>
          
          <div class="code-block">
            <h4>å®Œæ•´Transformerå®ç°</h4>
            <pre><code>class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout):
        super(Transformer, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        
        # è¯åµŒå…¥å±‚
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(d_model, max_len)
        
        # ç¼–ç å™¨å’Œè§£ç å™¨å±‚
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)
        ])
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.dropout = nn.Dropout(dropout)
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        # æºåºåˆ—åµŒå…¥å’Œä½ç½®ç¼–ç 
        src_embedding = self.dropout(self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model)))
        
        # ç›®æ ‡åºåˆ—åµŒå…¥å’Œä½ç½®ç¼–ç 
        tgt_embedding = self.dropout(self.positional_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model)))
        
        # ç¼–ç å™¨å¤„ç†
        enc_output = src_embedding
        for enc_layer in self.encoder_layers:
            enc_output = enc_layer(enc_output, src_mask)
        
        # è§£ç å™¨å¤„ç†
        dec_output = tgt_embedding
        for dec_layer in self.decoder_layers:
            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)
        
        # è¾“å‡ºå±‚
        output = self.fc_out(dec_output)
        return output</code></pre>
          </div>
        </div>
      </div>
    </section>
  </div>
</template>

<script>
import AttentionVisualization from './AttentionVisualization.vue';

export default {
  name: 'TransformerArchitecture',
  components: {
    AttentionVisualization
  },
  data() {
    return {
      // é¢„å®šä¹‰çš„æ³¨æ„åŠ›æƒé‡ç”¨äºEChartså¯è§†åŒ–
      attentionWeights: [],
      // è®­ç»ƒæµç¨‹æ•°æ®
      trainingSteps: [
        { id: 1, title: 'æ•°æ®è¾“å…¥', desc: 'å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®' },
        { id: 2, title: 'TokenåŒ–', desc: 'æ–‡æœ¬è½¬ä¸ºæ•°å­—åºåˆ—' },
        { id: 3, title: 'ä½ç½®ç¼–ç ', desc: 'æ·»åŠ ä½ç½®ä¿¡æ¯' },
        { id: 4, title: 'æ¨¡å‹å¤„ç†', desc: 'ç¼–ç å™¨-è§£ç å™¨å¤„ç†' },
        { id: 5, title: 'æŸå¤±è®¡ç®—', desc: 'é¢„æµ‹ä¸å®é™…æ¯”è¾ƒ' },
        { id: 6, title: 'å‚æ•°æ›´æ–°', desc: 'åå‘ä¼ æ’­ä¼˜åŒ–' }
      ],
      // æ¨ç†æµç¨‹æ•°æ®
      inferenceSteps: [
        { icon: 'ğŸ“¥', title: 'è¾“å…¥ç¼–ç ', desc: 'è¾“å…¥åºåˆ—é€šè¿‡ç¼–ç å™¨ç¼–ç ä¸ºè¡¨ç¤ºå‘é‡' },
        { icon: 'ğŸ”„', title: 'è‡ªå›å½’ç”Ÿæˆ', desc: 'è§£ç å™¨é€æ­¥ç”Ÿæˆè¾“å‡ºï¼Œæ¯æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ªtoken' },
        { icon: 'ğŸ“Š', title: 'æ¦‚ç‡è®¡ç®—', desc: 'è®¡ç®—è¯æ±‡è¡¨ä¸­æ¯ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ' },
        { icon: 'ğŸ¯', title: 'é‡‡æ ·ç­–ç•¥', desc: 'ä½¿ç”¨è´ªå©ªæœç´¢ã€æŸæœç´¢æˆ–éšæœºé‡‡æ ·é€‰æ‹©è¾“å‡ºtoken' }
      ],
      // å½“å‰è®­ç»ƒæ­¥éª¤
      currentTrainingStep: -1,
      // å½“å‰æ¨ç†æ­¥éª¤
      currentInferenceStep: -1,
      // è®­ç»ƒè¿›åº¦ç™¾åˆ†æ¯”
      trainingProgress: 0,
      // æ¨ç†è¿›åº¦ç™¾åˆ†æ¯”
      inferenceProgress: 0,
      // åŠ¨ç”»çŠ¶æ€
      isTrainingActive: false,
      isInferenceActive: false,
      // å®šæ—¶å™¨å¼•ç”¨
      trainingTimer: null,
      inferenceTimer: null
    };
  },
  mounted() {
    // ç”Ÿæˆæ¨¡æ‹Ÿçš„æ³¨æ„åŠ›æƒé‡æ•°æ®
    this.generateAttentionWeights();
  },
  beforeUnmount() {
    // æ¸…ç†å®šæ—¶å™¨
    if (this.trainingTimer) {
      clearInterval(this.trainingTimer);
    }
    if (this.inferenceTimer) {
      clearInterval(this.inferenceTimer);
    }
  },
  methods: {
    // ç”ŸæˆEChartséœ€è¦çš„æ³¨æ„åŠ›æƒé‡æ•°æ®
    generateAttentionWeights() {
      const weights = [];
      const sourceTokens = ['The', 'Transformer', 'model', 'is', 'powerful'];
      const targetTokens = ['Le', 'modÃ¨le', 'Transformer', 'est', 'puissant'];

      for (let i = 0; i < sourceTokens.length; i++) {
        for (let j = 0; j < targetTokens.length; j++) {
          // ç”Ÿæˆæ¨¡æ‹Ÿçš„æ³¨æ„åŠ›æƒé‡
          let weight;
          if (i === j) {
            weight = 0.7 + Math.random() * 0.3; // å¯¹è§’çº¿ä½ç½®æƒé‡è¾ƒé«˜
          } else if (Math.abs(i - j) === 1) {
            weight = 0.3 + Math.random() * 0.4; // é‚»è¿‘ä½ç½®æƒé‡ä¸­ç­‰
          } else {
            weight = Math.random() * 0.3; // å…¶ä»–ä½ç½®æƒé‡è¾ƒä½
          }
          weights.push([i, j, parseFloat(weight.toFixed(3))]);
        }
      }
      this.attentionWeights = weights;
    },

    // å¼€å§‹è®­ç»ƒåŠ¨ç”»
    startTrainingAnimation() {
      if (this.isTrainingActive) {
        this.pauseTrainingAnimation();
        return;
      }

      this.isTrainingActive = true;
      this.currentTrainingStep = 0;
      this.trainingProgress = 0;

      // æ¸…é™¤ä¹‹å‰çš„å®šæ—¶å™¨
      if (this.trainingTimer) {
        clearInterval(this.trainingTimer);
      }

      this.trainingTimer = setInterval(() => {
        if (this.currentTrainingStep < this.trainingSteps.length - 1) {
          this.currentTrainingStep++;
          this.trainingProgress = ((this.currentTrainingStep + 1) / this.trainingSteps.length) * 100;
        } else {
          // åŠ¨ç”»å®Œæˆ
          this.pauseTrainingAnimation();
        }
      }, 1500);
    },

    // æš‚åœè®­ç»ƒåŠ¨ç”»
    pauseTrainingAnimation() {
      this.isTrainingActive = false;
      if (this.trainingTimer) {
        clearInterval(this.trainingTimer);
        this.trainingTimer = null;
      }
    },

    // å¼€å§‹æ¨ç†åŠ¨ç”»
    startInferenceAnimation() {
      if (this.isInferenceActive) {
        this.pauseInferenceAnimation();
        return;
      }

      this.isInferenceActive = true;
      this.currentInferenceStep = 0;
      this.inferenceProgress = 0;

      // æ¸…é™¤ä¹‹å‰çš„å®šæ—¶å™¨
      if (this.inferenceTimer) {
        clearInterval(this.inferenceTimer);
      }

      this.inferenceTimer = setInterval(() => {
        if (this.currentInferenceStep < this.inferenceSteps.length - 1) {
          this.currentInferenceStep++;
          this.inferenceProgress = ((this.currentInferenceStep + 1) / this.inferenceSteps.length) * 100;
        } else {
          // åŠ¨ç”»å®Œæˆ
          this.pauseInferenceAnimation();
        }
      }, 1500);
    },

    // æš‚åœæ¨ç†åŠ¨ç”»
    pauseInferenceAnimation() {
      this.isInferenceActive = false;
      if (this.inferenceTimer) {
        clearInterval(this.inferenceTimer);
        this.inferenceTimer = null;
      }
    },

    // é€‰æ‹©è®­ç»ƒæ­¥éª¤
    selectTrainingStep(index) {
      this.currentTrainingStep = index;
      this.trainingProgress = ((index + 1) / this.trainingSteps.length) * 100;
    },

    // é€‰æ‹©æ¨ç†æ­¥éª¤
    selectInferenceStep(index) {
      this.currentInferenceStep = index;
      this.inferenceProgress = ((index + 1) / this.inferenceSteps.length) * 100;
    }
  }
};
</script>

<style scoped>
.transformer-container {
  max-width: 1400px;
  margin: 0 auto;
  padding: 30px;
  color: #e0e0e0;
  background: rgba(10, 14, 39, 0.8);
  border: 1px solid rgba(0, 242, 255, 0.2);
  border-radius: 16px;
  backdrop-filter: blur(10px);
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
}

.transformer-title {
  text-align: center;
  font-size: 38px;
  color: #00f2ff;
  margin-bottom: 15px;
  font-weight: 700;
  text-shadow: 0 0 15px rgba(0, 242, 255, 0.5);
}

.transformer-subtitle {
  text-align: center;
  font-size: 18px;
  color: #8899aa;
  margin-bottom: 40px;
}

.section {
  margin-bottom: 40px;
  padding: 25px;
  background: rgba(0, 0, 0, 0.2);
  border: 1px solid rgba(0, 242, 255, 0.1);
  border-radius: 12px;
  box-shadow: 0 4px 16px rgba(0, 0, 0, 0.2);
}

.section h3 {
  font-size: 26px;
  color: #00f2ff;
  margin-bottom: 20px;
  font-weight: 600;
}

.content {
  line-height: 1.8;
}

.content p {
  margin-bottom: 20px;
  font-size: 16px;
}

.architecture-diagram {
  margin-top: 20px;
}

.diagram-container {
  background: rgba(0, 0, 0, 0.3);
  padding: 20px;
  border-radius: 8px;
  overflow-x: auto;
}

.transformer-flow {
  display: flex;
  justify-content: space-between;
  align-items: center;
  gap: 20px;
  min-width: 1000px;
}

.input-section, .output-section {
  text-align: center;
  padding: 15px;
  background: rgba(0, 60, 120, 0.4);
  border: 1px solid rgba(0, 150, 255, 0.4);
  border-radius: 8px;
  min-width: 150px;
}

.input-tokens, .output-tokens {
  display: flex;
  flex-direction: column;
  gap: 8px;
  margin-bottom: 10px;
}

.token {
  padding: 8px;
  background: rgba(100, 150, 255, 0.3);
  border: 1px solid rgba(100, 150, 255, 0.5);
  border-radius: 4px;
  font-weight: bold;
}

.input-label, .output-label {
  font-size: 14px;
  color: #4dabf7;
}

.encoder-stack, .decoder-stack {
  flex: 1;
  min-width: 200px;
}

.encoder-header, .decoder-header {
  text-align: center;
  color: #51cf66;
  margin-bottom: 15px;
  font-size: 18px;
  font-weight: bold;
}

.encoder-blocks, .decoder-blocks {
  display: flex;
  flex-direction: column;
  gap: 10px;
}

.encoder-block, .decoder-block {
  padding: 15px;
  background: rgba(0, 40, 80, 0.4);
  border: 1px solid rgba(0, 120, 200, 0.4);
  border-radius: 8px;
}

.block-title {
  text-align: center;
  color: #ffd43b;
  margin-bottom: 10px;
  font-weight: bold;
}

.layer-name {
  text-align: center;
  font-size: 14px;
  color: #ff6b6b;
  margin-bottom: 8px;
}

.attention-layer,
.masked-attention-layer,
.cross-attention-layer,
.ffn-layer {
  margin-bottom: 10px;
  padding: 8px;
  background: rgba(80, 0, 120, 0.3);
  border: 1px solid rgba(120, 0, 200, 0.4);
  border-radius: 4px;
}

.attention-connections {
  display: grid;
  grid-template-columns: repeat(5, 1fr);
  grid-template-rows: repeat(5, 1fr);
  gap: 2px;
  width: 100px;
  height: 100px;
  margin: 0 auto;
}

.conn-cell {
  display: flex;
  align-items: center;
  justify-content: center;
  background: rgba(100, 100, 100, 0.3);
  border: 1px solid rgba(150, 150, 150, 0.3);
  transition: all 0.3s ease;
}

.conn-cell.active {
  background: rgba(50, 200, 100, 0.5);
  box-shadow: 0 0 5px rgba(50, 200, 100, 0.7);
}

.conn-dot {
  width: 6px;
  height: 6px;
  background: #00f2ff;
  border-radius: 50%;
}

.attention-chart {
  margin-top: 30px;
  padding: 20px;
  background: rgba(0, 0, 0, 0.3);
  border: 1px solid rgba(0, 242, 255, 0.2);
  border-radius: 12px;
}

.training-process, .inference-process {
  margin: 20px 0;
  padding: 15px;
  background: rgba(0, 30, 60, 0.3);
  border-radius: 8px;
}

.process-visualization {
  padding: 15px;
  background: rgba(0, 40, 80, 0.4);
  border-radius: 8px;
  margin-bottom: 15px;
}

.data-flow {
  display: flex;
  flex-direction: column;
  gap: 10px;
}

.data-step {
  padding: 10px;
  background: rgba(100, 150, 255, 0.2);
  border: 1px solid rgba(100, 150, 255, 0.3);
  border-radius: 4px;
  text-align: left;
}

.training-details, .inference-details {
  margin-top: 15px;
}

.training-details ul, .inference-details ul {
  padding-left: 20px;
}

.training-details li, .inference-details li {
  margin-bottom: 10px;
}

/* æ—¶é—´çº¿æ ·å¼ */
.timeline-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 20px;
  padding-bottom: 15px;
  border-bottom: 1px solid rgba(0, 242, 255, 0.2);
}

.timeline-header h4 {
  color: #00f2ff;
  font-size: 20px;
  margin: 0;
}

.timeline-controls {
  display: flex;
  gap: 10px;
}

.control-btn {
  padding: 8px 16px;
  background: rgba(0, 150, 255, 0.3);
  border: 1px solid rgba(0, 150, 255, 0.5);
  border-radius: 6px;
  color: #00f2ff;
  font-size: 14px;
  cursor: pointer;
  transition: all 0.3s ease;
}

.control-btn:hover {
  background: rgba(0, 150, 255, 0.5);
  box-shadow: 0 0 15px rgba(0, 150, 255, 0.6);
  transform: translateY(-2px);
}

.timeline-container {
  position: relative;
  padding: 40px 0;
  margin: 20px 0;
  overflow-x: auto;
}

.timeline-line {
  position: absolute;
  top: 50%;
  left: 5%;
  right: 5%;
  height: 4px;
  background: linear-gradient(90deg, rgba(0, 242, 255, 0.3), rgba(0, 242, 255, 0.6));
  border-radius: 2px;
  z-index: 1;
}

.timeline-progress {
  position: absolute;
  top: 0;
  left: 0;
  height: 100%;
  background: linear-gradient(90deg, #00f2ff, #00ff88);
  border-radius: 2px;
  transition: width 0.8s cubic-bezier(0.4, 0, 0.2, 1);
  box-shadow: 0 0 10px rgba(0, 242, 255, 0.8);
}

.timeline-steps {
  display: flex;
  justify-content: space-between;
  position: relative;
  z-index: 2;
  padding: 0 5%;
}

.timeline-step {
  flex: 1;
  position: relative;
  text-align: center;
  cursor: pointer;
  transition: all 0.3s ease;
  max-width: 180px;
}

.timeline-step:hover {
  transform: translateY(-5px);
}

.timeline-step.active .step-number {
  background: linear-gradient(135deg, #00f2ff, #00ff88);
  box-shadow: 0 0 20px rgba(0, 242, 255, 0.8);
  animation: stepPulse 2s infinite;
}

.timeline-step.completed .step-number {
  background: linear-gradient(135deg, #00ff88, #00cc66);
}

.timeline-step.completed .step-content h4 {
  color: #00ff88;
}

.step-number {
  width: 50px;
  height: 50px;
  margin: 0 auto 15px;
  background: rgba(0, 100, 150, 0.4);
  border: 2px solid rgba(0, 150, 255, 0.6);
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 20px;
  font-weight: bold;
  color: #00f2ff;
  transition: all 0.3s ease;
}

.step-icon {
  width: 50px;
  height: 50px;
  margin: 0 auto 15px;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 28px;
  transition: all 0.3s ease;
}

.timeline-step.active .step-icon {
  animation: stepPulse 2s infinite;
}

.step-content {
  background: rgba(0, 30, 60, 0.5);
  border: 1px solid rgba(0, 150, 255, 0.3);
  border-radius: 8px;
  padding: 15px 10px;
  transition: all 0.3s ease;
  backdrop-filter: blur(10px);
}

.timeline-step:hover .step-content {
  border-color: rgba(0, 242, 255, 0.6);
  box-shadow: 0 0 15px rgba(0, 150, 255, 0.4);
}

.step-content h4 {
  color: #4dabf7;
  margin: 0 0 8px 0;
  font-size: 16px;
  font-weight: 600;
}

.step-content p {
  margin: 0;
  font-size: 13px;
  color: #b0c4de;
  line-height: 1.5;
}

.step-connector {
  position: absolute;
  top: 25px;
  left: 100%;
  width: 100%;
  height: 4px;
  display: flex;
  align-items: center;
  overflow: hidden;
}

.connector-line {
  flex: 1;
  height: 100%;
  background: linear-gradient(90deg, transparent, rgba(0, 242, 255, 0.8), transparent);
  animation: dataFlow 2s infinite;
}

.connector-arrow {
  color: #00f2ff;
  font-size: 20px;
  margin-left: 5px;
  animation: arrowMove 1s infinite;
}

.timeline-step:last-child .step-connector {
  display: none;
}

/* åŠ¨ç”»æ•ˆæœ */
@keyframes dataFlow {
  0% {
    transform: translateX(-100%);
    opacity: 0;
  }
  50% {
    opacity: 1;
  }
  100% {
    transform: translateX(100%);
    opacity: 0;
  }
}

@keyframes stepPulse {
  0%, 100% {
    box-shadow: 0 0 10px rgba(0, 242, 255, 0.5);
  }
  50% {
    box-shadow: 0 0 25px rgba(0, 242, 255, 0.9);
  }
}

@keyframes arrowMove {
  0%, 100% {
    transform: translateX(0);
  }
  50% {
    transform: translateX(5px);
  }
}

.process-steps {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
  gap: 15px;
  margin: 20px 0;
}

.step {
  padding: 15px;
  background: rgba(0, 40, 80, 0.3);
  border: 1px solid rgba(0, 150, 200, 0.3);
  border-radius: 8px;
  text-align: center;
}

.step-icon {
  font-size: 24px;
  margin-bottom: 10px;
}

.step h4 {
  color: #4dabf7;
  margin: 10px 0 5px 0;
  font-size: 18px;
}

.component-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
  gap: 20px;
}

.component {
  padding: 20px;
  background: rgba(60, 0, 80, 0.2);
  border: 1px solid rgba(150, 100, 255, 0.3);
  border-radius: 8px;
}

.component h4 {
  color: #9775fa;
  margin-bottom: 10px;
  font-size: 18px;
}

.code-container {
  display: flex;
  flex-direction: column;
  gap: 20px;
}

.code-block {
  background: rgba(0, 0, 0, 0.4);
  border: 1px solid rgba(100, 200, 255, 0.3);
  border-radius: 8px;
  overflow: hidden;
}

.code-block h4 {
  padding: 12px 20px;
  background: rgba(0, 30, 60, 0.6);
  color: #ff6b6b;
  margin: 0;
  font-size: 18px;
}

pre {
  margin: 0;
  padding: 20px;
  overflow-x: auto;
  background: rgba(0, 0, 0, 0.5);
  color: #f8f8f2;
  font-size: 14px;
  line-height: 1.5;
}

@media (max-width: 1100px) {
  .transformer-flow {
    flex-direction: column;
    min-width: auto;
    gap: 30px;
  }

  .transformer-container {
    padding: 15px;
  }

  .transformer-title {
    font-size: 28px;
  }

  .process-steps, .inference-steps, .component-grid {
    grid-template-columns: 1fr;
  }

  .attention-grid {
    max-width: 100%;
    overflow-x: auto;
  }

  .timeline-steps {
    overflow-x: auto;
    padding-bottom: 20px;
  }

  .timeline-step {
    min-width: 150px;
  }
}

@media (max-width: 768px) {
  .timeline-header {
    flex-direction: column;
    gap: 15px;
    align-items: flex-start;
  }

  .timeline-container {
    padding: 30px 0;
  }

  .timeline-line {
    left: 10%;
    right: 10%;
  }

  .timeline-steps {
    padding: 0 10%;
  }

  .timeline-step {
    min-width: 130px;
  }

  .step-number {
    width: 40px;
    height: 40px;
    font-size: 16px;
  }

  .step-icon {
    width: 40px;
    height: 40px;
    font-size: 24px;
  }

  .step-content {
    padding: 12px 8px;
  }

  .step-content h4 {
    font-size: 14px;
  }

  .step-content p {
    font-size: 12px;
  }
}
</style>